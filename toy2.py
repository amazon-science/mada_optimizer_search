
 

# -*- coding: utf-8 -*-
"""CNN MNIST.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/14YB2h3vxvxOanJc1yDDEqw0O4atiz7Rm
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np

from PIL import Image
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import gdtuo



torch.manual_seed(2)
torch.cuda.manual_seed_all(2)

def im_convert(tensor):
  image = tensor.to("cpu").clone().detach()
  image = image.numpy().squeeze()
  return image
class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.ln1 = nn.Linear(100,100)

    def forward(self, x):
        x = self.ln1(x)
        return x
    
class Net(nn.Module):
  def __init__(self):
    super(Net,self).__init__()
    self.transformer = nn.ModuleDict(dict(
        conv1 = nn.Conv2d(1,10,kernel_size=5,stride=1),
        conv2 = nn.Conv2d(10,10,kernel_size=5,stride=1),
        h = nn.ModuleList([Block(10) for _ in range(1)]),
        pool = nn.MaxPool2d(kernel_size=2,stride=2), #2x2 maxpool
        dropout = nn.Dropout(0.2),
        fc1 = nn.Linear(4*4*10,100)
    ))
    
    self.conv1 = nn.Conv2d(1,10,kernel_size=5,stride=1)
    self.conv2 = nn.Conv2d(10,10,kernel_size=5,stride=1)
    self.pool = nn.MaxPool2d(kernel_size=2,stride=2) #2x2 maxpool
    self.fc1 = nn.Linear(4*4*10,100)
    self.fc2 = nn.Linear(100,10)

    #self.apply(self._init_weights)
    # apply special scaled init to the residual projections, per GPT-2 paper
    for pn, p in self.named_parameters():
        if pn.endswith('.weight'):
            torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * 5))
  
  def forward(self,x):
    
    x = F.relu(self.transformer.conv1(x)) #24x24x10
    x = self.transformer.dropout(x)
    x = self.transformer.pool(x) #12x12x10
    x = F.relu(self.transformer.conv2(x)) #8x8x10
    x = self.transformer.pool(x) #4x4x10    
    x = x.view(-1, 4*4*10) #flattening
    x = F.relu(self.transformer.fc1(x))
    for block in self.transformer.h:
       x = block(x)
    x = self.fc2(x)
    return x

train_ds = datasets.MNIST('../data',train=True,download=True, transform=transforms.Compose([transforms.ToTensor()]))
batch_size = 100
validation_split = .1
shuffle_dataset = True
random_seed= 2

# Creating data indices for training and validation splits:
dataset_size = len(train_ds)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
if shuffle_dataset :
    np.random.seed(random_seed)
    np.random.shuffle(indices)
train_indices, val_indices = indices[split:], indices[:split]
# Creating PT data samplers and loaders:
train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)
valid_sampler = torch.utils.data.SubsetRandomSampler(val_indices)


train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, 
                                           sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size,
                                                sampler=valid_sampler)

test_loader = torch.utils.data.DataLoader(
    datasets.MNIST('../data',train=False,download=True,
      transform=transforms.Compose([transforms.ToTensor()])),batch_size=batch_size,shuffle=True)

# fig, ax = plt.subplots(nrows=2,ncols=3)

# i=0
# for row in ax:
#   for col in row:
#     col.imshow(im_convert(train_loader.dataset[i][0]))
#     col.set_title("digit "+str(train_loader.dataset[i][1]))
#     col.axis("off")
#     i+=1

model = Net().cuda()
optimizer = optim.Adam(model.parameters(),lr=0.001)
criterion = nn.CrossEntropyLoss()

train_errors = []
train_acc = []
val_errors = []
val_acc = []
n_train = len(train_loader)*batch_size
n_val = len(validation_loader)*batch_size

optim = gdtuo.Adam(alpha = 1e-4, optimizer=gdtuo.SGD(1e-5))

mw = gdtuo.ModuleWrapper(model, optimizer=optim)
mw.initialize()
for i in range(10):
  total_loss = 0
  total_acc = 0  
  c = 0
  for images,labels in train_loader:
    mw.begin() 
    images = images.cuda()
    labels = labels.cuda()

    mw.zero_grad()
    output = model(images)
    loss = criterion(output,labels)
    loss.backward(create_graph=True) # important! use create_graph=True
    mw.step()

    # optimizer.zero_grad()
    # output = model(images)
    # loss = criterion(output,labels)
    # loss.backward()
    # optimizer.step()
    
    total_loss+=loss.item()
    total_acc+=torch.sum(torch.max(output,dim=1)[1]==labels).item()*1.0    
    c+=1
    
  
  #validation
  
  total_loss_val = 0
  total_acc_val = 0
  c = 0
  for images,labels in validation_loader:
    images = images.cuda()
    labels = labels.cuda()
    output = model(images)
    loss = criterion(output,labels)
    
    total_loss_val +=loss.item()
    total_acc_val +=torch.sum(torch.max(output,dim=1)[1]==labels).item()*1.0
    c+=1
  
  train_errors.append(total_loss/n_train)
  train_acc.append(total_acc/n_train)
  val_errors.append(total_loss_val/n_val)
  val_acc.append(total_acc_val/n_val)
  print("Train accuracy :",total_acc/n_train)
  
print("Trainig complete")

# fig, ax = plt.subplots(nrows=1, ncols=2)
# ax[0].plot(train_errors, 'r',label="Train")
# ax[0].plot(val_errors, 'g', label="Validation")
# ax[0].set_title("Grafik error")
# ax[0].set_ylabel("Error (cross-entropy)")
# ax[0].set_xlabel("Epoch")
# ax[0].legend()
# ax[1].plot(train_acc, 'r',label="Train")
# ax[1].plot(val_acc, 'g', label="Validation")
# ax[1].set_title("Grafik akurasi")
# ax[1].set_ylabel("Accuracy")
# ax[1].set_xlabel("Epoch")
# ax[1].legend()
# plt.show()

total_acc = 0
for images,labels in test_loader:
  images = images.cuda()
  labels = labels.cuda()
  output = model(images)
  total_acc+=torch.sum(torch.max(output,dim=1)[1]==labels).item()*1.0

print("Test accuracy :",total_acc/len(test_loader.dataset))
